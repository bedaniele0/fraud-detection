{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Pipeline ETL con Dask - VersiÃ³n Limpia\n",
    "=====================================\n",
    "\n",
    "**Autor**: Ing. Daniel Varela Perez  \n",
    "**Email**: bedaniele0@gmail.com  \n",
    "**TelÃ©fono**: +52 55 4189 3428  \n",
    "**Fecha**: 24 de Septiembre, 2025  \n",
    "\n",
    "**Objetivo**: Pipeline ETL limpio y funcional para procesamiento distribuido de datos de fraude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ PIPELINE ETL LIMPIO - DETECCIÃ“N DE FRAUDE\n",
      "ğŸ“Š Desarrollado por: Ing. Daniel Varela Perez\n",
      "ğŸ“… Fecha: 2025-09-24 15:09\n",
      "âœ… Setup completado\n",
      "ğŸ“Š Memoria - Setup inicial: 227.95 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pipeline ETL con Dask - VersiÃ³n Limpia\n",
    "=====================================\n",
    "Autor: Ing. Daniel Varela Perez\n",
    "\"\"\"\n",
    "# Imports principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# ML imports\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "# Crear directorios\n",
    "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# FunciÃ³n de monitoreo\n",
    "def monitor_memory_usage(stage=\"\"):\n",
    "    \"\"\"Monitorea memoria del proceso\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"ğŸ“Š Memoria - {stage}: {mem_mb:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error memoria: {e}\")\n",
    "\n",
    "print(\"ğŸš€ PIPELINE ETL LIMPIO - DETECCIÃ“N DE FRAUDE\")\n",
    "print(f\"ğŸ“Š Desarrollado por: Ing. Daniel Varela Perez\")\n",
    "print(f\"ğŸ“… Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"âœ… Setup completado\")\n",
    "monitor_memory_usage(\"Setup inicial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos con Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ CARGANDO DATOS...\n",
      "âœ… Datos cargados\n",
      "ğŸ“ Dimensiones: 3 particiones\n",
      "ğŸ“‹ Columnas: 31\n",
      "[########################################] | 100% Completed | 1.27 ss\n",
      "[########################################] | 100% Completed | 1.17 ss\n",
      "ğŸ“Š Total transacciones: 284,807\n",
      "ğŸš¨ Total fraudes: 492.0 (0.1727%)\n",
      "ğŸ“Š Memoria - Datos cargados: 536.68 MB\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset con Dask\n",
    "print(\"ğŸ“‚ CARGANDO DATOS...\")\n",
    "\n",
    "data_path = '../data/raw/creditcard.csv'\n",
    "ddf = dd.read_csv(data_path,\n",
    "                  blocksize='50MB',\n",
    "                  dtype={'Time': 'float64'},\n",
    "                  assume_missing=True)\n",
    "\n",
    "print(f\"âœ… Datos cargados\")\n",
    "print(f\"ğŸ“ Dimensiones: {ddf.npartitions} particiones\")\n",
    "print(f\"ğŸ“‹ Columnas: {len(ddf.columns)}\")\n",
    "\n",
    "# Info bÃ¡sica\n",
    "with ProgressBar():\n",
    "    total_rows = len(ddf)\n",
    "    fraud_count = ddf['Class'].sum().compute()\n",
    "    \n",
    "print(f\"ğŸ“Š Total transacciones: {total_rows:,}\")\n",
    "print(f\"ğŸš¨ Total fraudes: {fraud_count:,} ({fraud_count/total_rows*100:.4f}%)\")\n",
    "\n",
    "monitor_memory_usage(\"Datos cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering BÃ¡sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feature-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ CREANDO FEATURES...\n",
      "âœ… Features creados\n",
      "ğŸ“Š Total columnas: 41\n",
      "ğŸ”§ Nuevas features: ['hour_from_start', 'day_from_start', 'amount_log', 'is_zero_amount', 'is_high_amount', 'amount_zscore', 'V1_x_V2', 'V3_x_V4', 'V_sum_main', 'V_mean_main']\n",
      "ğŸ“Š Memoria - Features creados: 536.76 MB\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering simple y efectivo\n",
    "print(\"âš¡ CREANDO FEATURES...\")\n",
    "\n",
    "def create_features(partition):\n",
    "    \"\"\"Crea features bÃ¡sicos por particiÃ³n\"\"\"\n",
    "    \n",
    "    # Features temporales\n",
    "    partition['hour_from_start'] = partition['Time'] / 3600\n",
    "    partition['day_from_start'] = partition['Time'] / 86400\n",
    "    \n",
    "    # Features de monto\n",
    "    partition['amount_log'] = np.log1p(partition['Amount'])\n",
    "    partition['is_zero_amount'] = (partition['Amount'] == 0).astype(int)\n",
    "    partition['is_high_amount'] = (partition['Amount'] > 1000).astype(int)\n",
    "    \n",
    "    # Z-score local\n",
    "    amount_mean = partition['Amount'].mean()\n",
    "    amount_std = partition['Amount'].std()\n",
    "    if amount_std > 0:\n",
    "        partition['amount_zscore'] = (partition['Amount'] - amount_mean) / amount_std\n",
    "    else:\n",
    "        partition['amount_zscore'] = 0\n",
    "    \n",
    "    # Interacciones V1-V4 (principales)\n",
    "    partition['V1_x_V2'] = partition['V1'] * partition['V2']\n",
    "    partition['V3_x_V4'] = partition['V3'] * partition['V4']\n",
    "    \n",
    "    # Agregaciones\n",
    "    v_cols_main = ['V1', 'V2', 'V3', 'V4', 'V5']\n",
    "    partition['V_sum_main'] = partition[v_cols_main].sum(axis=1)\n",
    "    partition['V_mean_main'] = partition[v_cols_main].mean(axis=1)\n",
    "    \n",
    "    return partition\n",
    "\n",
    "# Aplicar feature engineering\n",
    "with ProgressBar():\n",
    "    ddf_features = ddf.map_partitions(create_features)\n",
    "\n",
    "print(f\"âœ… Features creados\")\n",
    "print(f\"ğŸ“Š Total columnas: {len(ddf_features.columns)}\")\n",
    "\n",
    "# Nuevas features\n",
    "new_features = [col for col in ddf_features.columns if col not in ddf.columns]\n",
    "print(f\"ğŸ”§ Nuevas features: {new_features}\")\n",
    "\n",
    "monitor_memory_usage(\"Features creados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Split Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "temporal-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… CREANDO SPLIT TEMPORAL...\n",
      "[########################################] | 100% Completed | 1.41 ss\n",
      "ğŸ“Š Rango temporal: 48.0 horas\n",
      "ğŸ“Š Train hasta: 33.6h (70%)\n",
      "ğŸ“Š Val: 33.6h - 40.8h (15%)\n",
      "ğŸ“Š Test desde: 40.8h (15%)\n",
      "[########################################] | 100% Completed | 1.38 ss\n",
      "[########################################] | 100% Completed | 1.46 ss\n",
      "[########################################] | 100% Completed | 1.33 ss\n",
      "[########################################] | 100% Completed | 1.34 ss\n",
      "[########################################] | 100% Completed | 1.34 ss\n",
      "[########################################] | 100% Completed | 1.33 ss\n",
      "âœ… SPLITS CREADOS:\n",
      "â€¢ Train: 172,090 filas, 360.0 fraudes (0.2092%)\n",
      "â€¢ Val: 59,587 filas, 61.0 fraudes (0.1024%)\n",
      "â€¢ Test: 53,130 filas, 71.0 fraudes (0.1336%)\n",
      "ğŸ“Š Memoria - Splits creados: 463.61 MB\n"
     ]
    }
   ],
   "source": [
    "# Split temporal para evitar data leakage\n",
    "print(\"ğŸ“… CREANDO SPLIT TEMPORAL...\")\n",
    "\n",
    "with ProgressBar():\n",
    "    # Obtener rango temporal\n",
    "    time_stats = ddf_features['Time'].describe().compute()\n",
    "    min_time = time_stats['min']\n",
    "    max_time = time_stats['max']\n",
    "    time_range = max_time - min_time\n",
    "\n",
    "# Puntos de corte (70% train, 15% val, 15% test)\n",
    "train_cutoff = min_time + (time_range * 0.70)\n",
    "val_cutoff = min_time + (time_range * 0.85)\n",
    "\n",
    "print(f\"ğŸ“Š Rango temporal: {time_range/3600:.1f} horas\")\n",
    "print(f\"ğŸ“Š Train hasta: {train_cutoff/3600:.1f}h (70%)\")\n",
    "print(f\"ğŸ“Š Val: {train_cutoff/3600:.1f}h - {val_cutoff/3600:.1f}h (15%)\")\n",
    "print(f\"ğŸ“Š Test desde: {val_cutoff/3600:.1f}h (15%)\")\n",
    "\n",
    "# Crear splits\n",
    "train_ddf = ddf_features[ddf_features['Time'] < train_cutoff]\n",
    "val_ddf = ddf_features[(ddf_features['Time'] >= train_cutoff) & (ddf_features['Time'] < val_cutoff)]\n",
    "test_ddf = ddf_features[ddf_features['Time'] >= val_cutoff]\n",
    "\n",
    "# Verificar tamaÃ±os\n",
    "with ProgressBar():\n",
    "    train_size = len(train_ddf)\n",
    "    val_size = len(val_ddf)\n",
    "    test_size = len(test_ddf)\n",
    "    \n",
    "    train_fraud = train_ddf['Class'].sum().compute()\n",
    "    val_fraud = val_ddf['Class'].sum().compute()\n",
    "    test_fraud = test_ddf['Class'].sum().compute()\n",
    "\n",
    "print(f\"âœ… SPLITS CREADOS:\")\n",
    "print(f\"â€¢ Train: {train_size:,} filas, {train_fraud} fraudes ({train_fraud/train_size*100:.4f}%)\")\n",
    "print(f\"â€¢ Val: {val_size:,} filas, {val_fraud} fraudes ({val_fraud/val_size*100:.4f}%)\")\n",
    "print(f\"â€¢ Test: {test_size:,} filas, {test_fraud} fraudes ({test_fraud/test_size*100:.4f}%)\")\n",
    "\n",
    "monitor_memory_usage(\"Splits creados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. PreparaciÃ³n para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ml-prep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ PREPARANDO PARA ML...\n",
      "ğŸ“‹ Features para ML: 39\n",
      "ğŸ¯ Target: Class\n",
      "ğŸ“ Creando scaler...\n",
      "[########################################] | 100% Completed | 1.45 ss\n",
      "âœ… Scaler ajustado con muestra de 17,209 registros\n",
      "ğŸ“Š Memoria - Scaler preparado: 427.25 MB\n"
     ]
    }
   ],
   "source": [
    "# Preparar features para ML\n",
    "print(\"ğŸ¯ PREPARANDO PARA ML...\")\n",
    "\n",
    "# Columnas a excluir\n",
    "exclude_cols = ['Time', 'Class']\n",
    "feature_columns = [col for col in ddf_features.columns if col not in exclude_cols]\n",
    "target_column = 'Class'\n",
    "\n",
    "print(f\"ğŸ“‹ Features para ML: {len(feature_columns)}\")\n",
    "print(f\"ğŸ¯ Target: {target_column}\")\n",
    "\n",
    "# Crear scaler con muestra del train\n",
    "print(\"ğŸ“ Creando scaler...\")\n",
    "with ProgressBar():\n",
    "    train_sample = train_ddf[feature_columns].sample(frac=0.1).compute()\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_sample)\n",
    "\n",
    "print(f\"âœ… Scaler ajustado con muestra de {len(train_sample):,} registros\")\n",
    "\n",
    "# FunciÃ³n para normalizar particiones\n",
    "def normalize_partition(partition, feature_cols):\n",
    "    \"\"\"Normaliza features en una particiÃ³n\"\"\"\n",
    "    partition_norm = partition.copy()\n",
    "    if len(partition_norm) > 0:\n",
    "        normalized_features = scaler.transform(partition_norm[feature_cols])\n",
    "        partition_norm[feature_cols] = normalized_features\n",
    "    return partition_norm\n",
    "\n",
    "monitor_memory_usage(\"Scaler preparado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Demo de Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "balancing-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ DEMO BALANCEADO...\n",
      "[########################################] | 100% Completed | 2.84 ss\n",
      "ğŸ“Š Muestra para demo: 3,442 registros\n",
      "ğŸš¨ Fraudes en muestra: 4.0\n",
      "âš ï¸ Insuficientes fraudes para demo de balanceado\n",
      "ğŸ“Š Memoria - Demo balanceado: 498.43 MB\n"
     ]
    }
   ],
   "source": [
    "# Demo de balanceado con SMOTE en muestra pequeÃ±a\n",
    "print(\"âš–ï¸ DEMO BALANCEADO...\")\n",
    "\n",
    "# Obtener muestra para demo\n",
    "with ProgressBar():\n",
    "    demo_sample = train_ddf.sample(frac=0.02).compute()  # 2% del train\n",
    "\n",
    "print(f\"ğŸ“Š Muestra para demo: {len(demo_sample):,} registros\")\n",
    "print(f\"ğŸš¨ Fraudes en muestra: {demo_sample['Class'].sum()}\")\n",
    "\n",
    "if demo_sample['Class'].sum() >= 5:  # Solo si hay suficientes fraudes\n",
    "    \n",
    "    X_demo = demo_sample[feature_columns]\n",
    "    y_demo = demo_sample[target_column]\n",
    "    \n",
    "    # EstadÃ­sticas antes\n",
    "    fraud_before = y_demo.sum()\n",
    "    normal_before = len(y_demo) - fraud_before\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ANTES DEL BALANCEADO:\")\n",
    "    print(f\"â€¢ Normal: {normal_before:,} ({normal_before/len(y_demo)*100:.2f}%)\")\n",
    "    print(f\"â€¢ Fraude: {fraud_before:,} ({fraud_before/len(y_demo)*100:.2f}%)\")\n",
    "    print(f\"â€¢ Ratio: {normal_before/fraud_before:.1f}:1\")\n",
    "    \n",
    "    try:\n",
    "        # Aplicar SMOTE\n",
    "        smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X_demo, y_demo)\n",
    "        \n",
    "        # EstadÃ­sticas despuÃ©s\n",
    "        fraud_after = y_balanced.sum()\n",
    "        normal_after = len(y_balanced) - fraud_after\n",
    "        \n",
    "        print(f\"\\nğŸ“Š DESPUÃ‰S DEL BALANCEADO:\")\n",
    "        print(f\"â€¢ Normal: {normal_after:,} ({normal_after/len(y_balanced)*100:.2f}%)\")\n",
    "        print(f\"â€¢ Fraude: {fraud_after:,} ({fraud_after/len(y_balanced)*100:.2f}%)\")\n",
    "        print(f\"â€¢ Ratio: {normal_after/fraud_after:.1f}:1\")\n",
    "        print(f\"â€¢ TamaÃ±o final: {len(y_balanced):,} ({len(y_balanced)/len(y_demo):.1f}x)\")\n",
    "        \n",
    "        print(\"âœ… SMOTE demo completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error en SMOTE: {e}\")\n",
    "        print(\"Continuando sin balanceado...\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ Insuficientes fraudes para demo de balanceado\")\n",
    "\n",
    "monitor_memory_usage(\"Demo balanceado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Guardado de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save-datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ GUARDANDO DATASETS...\n",
      "\n",
      "ğŸ’¾ Procesando train...\n",
      "[########################################] | 100% Completed | 2.26 ss\n",
      "âœ… train: 56.2 MB (2.3s)\n",
      "\n",
      "ğŸ’¾ Procesando validation...\n",
      "[########################################] | 100% Completed | 1.95 ss\n",
      "âœ… validation: 19.4 MB (2.0s)\n",
      "\n",
      "ğŸ’¾ Procesando test...\n",
      "[########################################] | 100% Completed | 2.18 ss\n",
      "âœ… test: 17.2 MB (2.2s)\n",
      "\n",
      "ğŸ“Š RESUMEN GUARDADO:\n",
      "â€¢ Total datasets: 3\n",
      "â€¢ TamaÃ±o total: 92.7 MB\n",
      "â€¢ Tiempo total: 6.4s\n",
      "ğŸ“Š Memoria - Datasets guardados: 487.53 MB\n"
     ]
    }
   ],
   "source": [
    "# Guardar datasets procesados\n",
    "print(\"ğŸ’¾ GUARDANDO DATASETS...\")\n",
    "\n",
    "# Normalizar y guardar cada split\n",
    "datasets_info = {}\n",
    "\n",
    "for split_name, ddf_split in [('train', train_ddf), ('validation', val_ddf), ('test', test_ddf)]:\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Procesando {split_name}...\")\n",
    "    \n",
    "    # Normalizar\n",
    "    with ProgressBar():\n",
    "        ddf_normalized = ddf_split.map_partitions(\n",
    "            lambda x: normalize_partition(x, feature_columns)\n",
    "        )\n",
    "    \n",
    "    # Seleccionar columnas finales\n",
    "    final_columns = feature_columns + [target_column, 'Time']\n",
    "    ddf_final = ddf_normalized[final_columns]\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = f\"../data/processed/{split_name}_clean.parquet\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ProgressBar():\n",
    "            ddf_final.to_parquet(output_path, write_index=False, compression='snappy')\n",
    "        \n",
    "        save_time = time.time() - start_time\n",
    "        \n",
    "        # Verificar guardado\n",
    "        if Path(output_path).exists():\n",
    "            total_size = sum(f.stat().st_size for f in Path(output_path).rglob('*.parquet'))\n",
    "            size_mb = total_size / (1024**2)\n",
    "            \n",
    "            print(f\"âœ… {split_name}: {size_mb:.1f} MB ({save_time:.1f}s)\")\n",
    "            \n",
    "            datasets_info[split_name] = {\n",
    "                'path': output_path,\n",
    "                'size_mb': size_mb,\n",
    "                'save_time': save_time\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ {split_name}: Error - archivo no encontrado\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {split_name}: Error - {str(e)[:100]}...\")\n",
    "\n",
    "# Resumen\n",
    "total_size = sum(info.get('size_mb', 0) for info in datasets_info.values())\n",
    "total_time = sum(info.get('save_time', 0) for info in datasets_info.values())\n",
    "\n",
    "print(f\"\\nğŸ“Š RESUMEN GUARDADO:\")\n",
    "print(f\"â€¢ Total datasets: {len(datasets_info)}\")\n",
    "print(f\"â€¢ TamaÃ±o total: {total_size:.1f} MB\")\n",
    "print(f\"â€¢ Tiempo total: {total_time:.1f}s\")\n",
    "\n",
    "monitor_memory_usage(\"Datasets guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. SerializaciÃ³n del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serialize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ SERIALIZANDO PIPELINE...\n",
      "âœ… Scaler guardado: ../data/processed/scaler_clean.pkl\n",
      "âœ… Metadatos guardados: ../data/processed/pipeline_metadata_clean.json\n",
      "âœ… Script de carga: ../data/processed/load_clean_pipeline.py\n",
      "\n",
      "ğŸ“¦ PIPELINE SERIALIZADO:\n",
      "â€¢ Scaler: scaler_clean.pkl\n",
      "â€¢ Metadatos: pipeline_metadata_clean.json\n",
      "â€¢ Loader: load_clean_pipeline.py\n",
      "ğŸ“Š Memoria - Pipeline serializado: 487.54 MB\n"
     ]
    }
   ],
   "source": [
    "# Serializar componentes del pipeline\n",
    "print(\"ğŸ“¦ SERIALIZANDO PIPELINE...\")\n",
    "\n",
    "base_path = '../data/processed'\n",
    "\n",
    "# 1. Guardar scaler\n",
    "scaler_path = f\"{base_path}/scaler_clean.pkl\"\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"âœ… Scaler guardado: {scaler_path}\")\n",
    "\n",
    "# 2. Guardar metadatos\n",
    "pipeline_metadata = {\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'analyst': 'Ing. Daniel Varela Perez',\n",
    "    'contact': 'bedaniele0@gmail.com',\n",
    "    'phone': '+52 55 4189 3428',\n",
    "    'version': 'clean',\n",
    "    \n",
    "    'feature_info': {\n",
    "        'total_features': len(feature_columns),\n",
    "        'feature_columns': feature_columns,\n",
    "        'target_column': target_column,\n",
    "        'scaler_type': 'RobustScaler'\n",
    "    },\n",
    "    \n",
    "    'split_info': {\n",
    "        'train_size': train_size,\n",
    "        'val_size': val_size,\n",
    "        'test_size': test_size,\n",
    "        'train_fraud': int(train_fraud),\n",
    "        'val_fraud': int(val_fraud),\n",
    "        'test_fraud': int(test_fraud)\n",
    "    },\n",
    "    \n",
    "    'datasets_info': datasets_info\n",
    "}\n",
    "\n",
    "metadata_path = f\"{base_path}/pipeline_metadata_clean.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(pipeline_metadata, f, indent=2)\n",
    "print(f\"âœ… Metadatos guardados: {metadata_path}\")\n",
    "\n",
    "# 3. Script de carga\n",
    "loader_script = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script para cargar pipeline ETL limpio\n",
    "Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "import pickle\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def load_clean_datasets():\n",
    "    \"\"\"Carga datasets limpios\"\"\"\n",
    "    train = dd.read_parquet('{base_path}/train_clean.parquet')\n",
    "    val = dd.read_parquet('{base_path}/validation_clean.parquet')\n",
    "    test = dd.read_parquet('{base_path}/test_clean.parquet')\n",
    "    return train, val, test\n",
    "\n",
    "def load_scaler():\n",
    "    \"\"\"Carga scaler\"\"\"\n",
    "    with open('{base_path}/scaler_clean.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_metadata():\n",
    "    \"\"\"Carga metadatos\"\"\"\n",
    "    with open('{base_path}/pipeline_metadata_clean.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Cargando pipeline ETL limpio...\")\n",
    "    train, val, test = load_clean_datasets()\n",
    "    scaler = load_scaler()\n",
    "    metadata = load_metadata()\n",
    "    \n",
    "    print(f\"Train: {{len(train):,}} filas\")\n",
    "    print(f\"Val: {{len(val):,}} filas\")\n",
    "    print(f\"Test: {{len(test):,}} filas\")\n",
    "    print(f\"Features: {{len(metadata['feature_info']['feature_columns'])}}\")\n",
    "    print(f\"Scaler: {{type(scaler).__name__}}\")\n",
    "'''\n",
    "\n",
    "loader_path = f\"{base_path}/load_clean_pipeline.py\"\n",
    "with open(loader_path, 'w') as f:\n",
    "    f.write(loader_script)\n",
    "print(f\"âœ… Script de carga: {loader_path}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ PIPELINE SERIALIZADO:\")\n",
    "print(f\"â€¢ Scaler: scaler_clean.pkl\")\n",
    "print(f\"â€¢ Metadatos: pipeline_metadata_clean.json\")\n",
    "print(f\"â€¢ Loader: load_clean_pipeline.py\")\n",
    "\n",
    "monitor_memory_usage(\"Pipeline serializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 8. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ RESUMEN PIPELINE ETL LIMPIO\n",
      "==================================================\n",
      "ğŸ“Š DATOS PROCESADOS:\n",
      "â€¢ Train: 172,090 filas (360.0 fraudes)\n",
      "â€¢ Validation: 59,587 filas (61.0 fraudes)\n",
      "â€¢ Test: 53,130 filas (71.0 fraudes)\n",
      "\n",
      "ğŸ”§ FEATURES CREADOS:\n",
      "â€¢ Total features: 39\n",
      "â€¢ Nuevas features: 10\n",
      "â€¢ NormalizaciÃ³n: RobustScaler\n",
      "\n",
      "ğŸ’¾ ARCHIVOS GENERADOS:\n",
      "â€¢ train_clean.parquet (56.2 MB)\n",
      "â€¢ validation_clean.parquet (19.4 MB)\n",
      "â€¢ test_clean.parquet (17.2 MB)\n",
      "\n",
      "âœ… CARACTERÃSTICAS:\n",
      "â€¢ âœ… Sin data leakage (split temporal)\n",
      "â€¢ âœ… Features normalizadas\n",
      "â€¢ âœ… Pipeline serializado\n",
      "â€¢ âœ… Demo SMOTE incluido\n",
      "â€¢ âœ… CÃ³digo limpio y funcional\n",
      "\n",
      "ğŸš€ LISTO PARA FASE 3: MODELADO ML\n",
      "ğŸ‘¨â€ğŸ’» Desarrollado por: Ing. Daniel Varela Perez\n",
      "ğŸ“§ bedaniele0@gmail.com | ğŸ“± +52 55 4189 3428\n",
      "==================================================\n",
      "ğŸ“Š Memoria - Pipeline completado: 487.54 MB\n"
     ]
    }
   ],
   "source": [
    "# Resumen final del pipeline limpio\n",
    "print(\"ğŸ¯ RESUMEN PIPELINE ETL LIMPIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"ğŸ“Š DATOS PROCESADOS:\")\n",
    "print(f\"â€¢ Train: {train_size:,} filas ({train_fraud} fraudes)\")\n",
    "print(f\"â€¢ Validation: {val_size:,} filas ({val_fraud} fraudes)\")\n",
    "print(f\"â€¢ Test: {test_size:,} filas ({test_fraud} fraudes)\")\n",
    "\n",
    "print(f\"\\nğŸ”§ FEATURES CREADOS:\")\n",
    "print(f\"â€¢ Total features: {len(feature_columns)}\")\n",
    "print(f\"â€¢ Nuevas features: {len(new_features)}\")\n",
    "print(f\"â€¢ NormalizaciÃ³n: RobustScaler\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ARCHIVOS GENERADOS:\")\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"â€¢ {name}_clean.parquet ({info['size_mb']:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nâœ… CARACTERÃSTICAS:\")\n",
    "print(f\"â€¢ âœ… Sin data leakage (split temporal)\")\n",
    "print(f\"â€¢ âœ… Features normalizadas\")\n",
    "print(f\"â€¢ âœ… Pipeline serializado\")\n",
    "print(f\"â€¢ âœ… Demo SMOTE incluido\")\n",
    "print(f\"â€¢ âœ… CÃ³digo limpio y funcional\")\n",
    "\n",
    "print(f\"\\nğŸš€ LISTO PARA FASE 3: MODELADO ML\")\n",
    "print(f\"ğŸ‘¨â€ğŸ’» Desarrollado por: Ing. Daniel Varela Perez\")\n",
    "print(f\"ğŸ“§ bedaniele0@gmail.com | ğŸ“± +52 55 4189 3428\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "monitor_memory_usage(\"Pipeline completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba78e13-2d0b-4dfe-b43a-3be5995d42c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
