{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Mejorar Recall - Detecci√≥n de Fraude\n",
    "\n",
    "**Autor**: Ing. Daniel Varela Perez  \n",
    "**Email**: bedaniele0@gmail.com  \n",
    "**Tel√©fono**: +52 55 4189 3428  \n",
    "**Fecha**: 24 de Septiembre, 2025\n",
    "\n",
    "**Objetivo**: Mejorar el recall del modelo manteniendo alta precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completados\n"
     ]
    }
   ],
   "source": [
    "# Setup b√°sico\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando modelo y datos...\n",
      "‚úÖ Modelo simple cargado\n",
      "Train: (172090, 39), Test: (53130, 39)\n",
      "Fraudes train: 360.0 (0.2092%)\n",
      "Fraudes test: 71.0 (0.1336%)\n",
      "‚úÖ Datos preparados\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo y datos\n",
    "print(\"üìÇ Cargando modelo y datos...\")\n",
    "\n",
    "# Cargar modelo anterior\n",
    "simple_model = joblib.load('../models/simple_fraud_model.pkl')\n",
    "print(\"‚úÖ Modelo simple cargado\")\n",
    "\n",
    "# Cargar datos\n",
    "train_df = pd.read_parquet('../data/processed/train_clean.parquet')\n",
    "test_df = pd.read_parquet('../data/processed/test_clean.parquet')\n",
    "\n",
    "# Preparar datos\n",
    "feature_cols = [col for col in train_df.columns if col not in ['Class', 'Time']]\n",
    "target_col = 'Class'\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Fraudes train: {y_train.sum()} ({y_train.sum()/len(y_train)*100:.4f}%)\")\n",
    "print(f\"Fraudes test: {y_test.sum()} ({y_test.sum()/len(y_test)*100:.4f}%)\")\n",
    "print(\"‚úÖ Datos preparados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baseline-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PERFORMANCE BASELINE\n",
      "==============================\n",
      "Precision: 1.0000\n",
      "Recall: 0.1268\n",
      "F1-Score: 0.2250\n",
      "Fraudes detectados: 9.0/71.0\n",
      "\n",
      "Matriz Confusi√≥n Baseline:\n",
      "[[53059     0]\n",
      " [   62     9]]\n",
      "‚úÖ Baseline evaluado\n"
     ]
    }
   ],
   "source": [
    "# Performance del modelo baseline\n",
    "print(\"üìä PERFORMANCE BASELINE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Predicciones baseline\n",
    "y_pred_baseline = simple_model.predict(X_test)\n",
    "y_proba_baseline = simple_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas baseline\n",
    "precision_baseline = precision_score(y_test, y_pred_baseline)\n",
    "recall_baseline = recall_score(y_test, y_pred_baseline)\n",
    "f1_baseline = f1_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Precision: {precision_baseline:.4f}\")\n",
    "print(f\"Recall: {recall_baseline:.4f}\")\n",
    "print(f\"F1-Score: {f1_baseline:.4f}\")\n",
    "print(f\"Fraudes detectados: {y_pred_baseline.sum()}/{y_test.sum()}\")\n",
    "\n",
    "# Matriz de confusi√≥n baseline\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "print(f\"\\nMatriz Confusi√≥n Baseline:\")\n",
    "print(cm_baseline)\n",
    "print(\"‚úÖ Baseline evaluado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "threshold-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ AJUSTANDO THRESHOLD PARA MEJOR RECALL\n",
      "=============================================\n",
      "Threshold 0.50: Precision=1.000, Recall=0.155, F1=0.268, Detectados=11/71\n",
      "Threshold 0.30: Precision=1.000, Recall=0.662, F1=0.797, Detectados=47/71\n",
      "Threshold 0.20: Precision=0.944, Recall=0.718, F1=0.816, Detectados=54/71\n",
      "Threshold 0.10: Precision=0.885, Recall=0.761, F1=0.818, Detectados=61/71\n",
      "Threshold 0.05: Precision=0.767, Recall=0.789, F1=0.778, Detectados=73/71\n",
      "Threshold 0.03: Precision=0.479, Recall=0.817, F1=0.604, Detectados=121/71\n",
      "Threshold 0.01: Precision=0.048, Recall=0.873, F1=0.091, Detectados=1299/71\n",
      "\n",
      "‚úÖ An√°lisis de thresholds completado\n"
     ]
    }
   ],
   "source": [
    "# Ajustar threshold para mejorar recall\n",
    "print(\"üéØ AJUSTANDO THRESHOLD PARA MEJOR RECALL\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Probar diferentes thresholds\n",
    "thresholds = [0.5, 0.3, 0.2, 0.1, 0.05, 0.03, 0.01]\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Predicciones con threshold personalizado\n",
    "    y_pred_thresh = (y_proba_baseline >= threshold).astype(int)\n",
    "    \n",
    "    # M√©tricas\n",
    "    precision = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    \n",
    "    # Fraudes detectados\n",
    "    detected = y_pred_thresh.sum()\n",
    "    true_frauds = y_test.sum()\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'detected': detected,\n",
    "        'true_frauds': int(true_frauds)\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold {threshold:4.2f}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, Detectados={detected}/{int(true_frauds)}\")\n",
    "\n",
    "# Convertir a DataFrame para an√°lisis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n‚úÖ An√°lisis de thresholds completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "best-threshold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ SELECCIONAR MEJOR THRESHOLD\n",
      "===================================\n",
      "üéØ MEJOR THRESHOLD: 0.1\n",
      "üìä M√âTRICAS:\n",
      "  ‚Ä¢ Precision: 0.8852\n",
      "  ‚Ä¢ Recall: 0.7606\n",
      "  ‚Ä¢ F1-Score: 0.8182\n",
      "  ‚Ä¢ Fraudes detectados: 61.0/71.0\n",
      "\n",
      "‚úÖ Mejor threshold seleccionado\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar mejor threshold balanceando precision y recall\n",
    "print(\"üèÜ SELECCIONAR MEJOR THRESHOLD\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Criterios para selecci√≥n:\n",
    "# 1. Recall > 50% (detectar al menos la mitad de fraudes)\n",
    "# 2. Precision > 10% (evitar demasiadas falsas alarmas)\n",
    "# 3. Maximizar F1-Score\n",
    "\n",
    "# Filtrar candidatos viables\n",
    "viable = results_df[\n",
    "    (results_df['recall'] >= 0.50) & \n",
    "    (results_df['precision'] >= 0.10)\n",
    "]\n",
    "\n",
    "if len(viable) > 0:\n",
    "    # Seleccionar el de mejor F1-Score\n",
    "    best_idx = viable['f1'].idxmax()\n",
    "    best_threshold = results_df.loc[best_idx, 'threshold']\n",
    "    best_result = results_df.loc[best_idx]\n",
    "else:\n",
    "    # Si no hay viables, seleccionar el de mejor recall con precision >= 5%\n",
    "    viable_relaxed = results_df[results_df['precision'] >= 0.05]\n",
    "    if len(viable_relaxed) > 0:\n",
    "        best_idx = viable_relaxed['recall'].idxmax()\n",
    "        best_threshold = results_df.loc[best_idx, 'threshold']\n",
    "        best_result = results_df.loc[best_idx]\n",
    "    else:\n",
    "        # Fallback: mejor F1 general\n",
    "        best_idx = results_df['f1'].idxmax()\n",
    "        best_threshold = results_df.loc[best_idx, 'threshold']\n",
    "        best_result = results_df.loc[best_idx]\n",
    "\n",
    "print(f\"üéØ MEJOR THRESHOLD: {best_threshold}\")\n",
    "print(f\"üìä M√âTRICAS:\")\n",
    "print(f\"  ‚Ä¢ Precision: {best_result['precision']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {best_result['recall']:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {best_result['f1']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Fraudes detectados: {best_result['detected']}/{best_result['true_frauds']}\")\n",
    "\n",
    "# Aplicar mejor threshold\n",
    "y_pred_best = (y_proba_baseline >= best_threshold).astype(int)\n",
    "print(\"\\n‚úÖ Mejor threshold seleccionado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "smote-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è ENTRENANDO MODELO CON SMOTE\n",
      "===================================\n",
      "üîÑ Aplicando SMOTE...\n",
      "Original: 172,090 muestras (360.0 fraudes)\n",
      "SMOTE: 343,460 muestras (171730.0 fraudes)\n",
      "Ratio despu√©s SMOTE: 1.0:1\n",
      "\n",
      "üöÄ Entrenando Random Forest con SMOTE...\n",
      "‚úÖ Modelo SMOTE entrenado\n"
     ]
    }
   ],
   "source": [
    "# Entrenar modelo con SMOTE para mejor recall\n",
    "print(\"‚öñÔ∏è ENTRENANDO MODELO CON SMOTE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Aplicar SMOTE\n",
    "print(\"üîÑ Aplicando SMOTE...\")\n",
    "smote = SMOTE(random_state=42, k_neighbors=min(5, int(y_train.sum())-1))\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original: {len(y_train):,} muestras ({y_train.sum()} fraudes)\")\n",
    "print(f\"SMOTE: {len(y_train_smote):,} muestras ({y_train_smote.sum()} fraudes)\")\n",
    "print(f\"Ratio despu√©s SMOTE: {(len(y_train_smote) - y_train_smote.sum()) / y_train_smote.sum():.1f}:1\")\n",
    "\n",
    "# Entrenar modelo con datos balanceados\n",
    "print(\"\\nüöÄ Entrenando Random Forest con SMOTE...\")\n",
    "rf_smote = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight=None,  # No necesario con SMOTE\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"‚úÖ Modelo SMOTE entrenado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "evaluate-smote",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä EVALUACI√ìN MODELO SMOTE\n",
      "==============================\n",
      "SMOTE Precision: 1.0000\n",
      "SMOTE Recall: 0.3521\n",
      "SMOTE F1-Score: 0.5208\n",
      "Fraudes detectados: 25.0/71.0\n",
      "\n",
      "Matriz Confusi√≥n SMOTE:\n",
      "[[53059     0]\n",
      " [   46    25]]\n",
      "\n",
      "‚úÖ Modelo SMOTE evaluado\n"
     ]
    }
   ],
   "source": [
    "# Evaluar modelo SMOTE\n",
    "print(\"üìä EVALUACI√ìN MODELO SMOTE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Predicciones modelo SMOTE\n",
    "y_pred_smote = rf_smote.predict(X_test)\n",
    "y_proba_smote = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas modelo SMOTE\n",
    "precision_smote = precision_score(y_test, y_pred_smote)\n",
    "recall_smote = recall_score(y_test, y_pred_smote)\n",
    "f1_smote = f1_score(y_test, y_pred_smote)\n",
    "\n",
    "print(f\"SMOTE Precision: {precision_smote:.4f}\")\n",
    "print(f\"SMOTE Recall: {recall_smote:.4f}\")\n",
    "print(f\"SMOTE F1-Score: {f1_smote:.4f}\")\n",
    "print(f\"Fraudes detectados: {y_pred_smote.sum()}/{y_test.sum()}\")\n",
    "\n",
    "# Matriz de confusi√≥n SMOTE\n",
    "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "print(f\"\\nMatriz Confusi√≥n SMOTE:\")\n",
    "print(cm_smote)\n",
    "print(\"\\n‚úÖ Modelo SMOTE evaluado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPARACI√ìN DE ENFOQUES\n",
      "========================================\n",
      "DEBUG - Columnas disponibles en comparison:\n",
      "['Enfoque', 'Precision', 'Recall', 'F1_Score', 'Fraudes_Detectados']\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "                    Enfoque  Precision    Recall  F1_Score  Fraudes_Detectados\n",
      "0  Baseline (threshold=0.5)   1.000000  0.126761  0.225000                 9.0\n",
      "1  Threshold Ajustado (0.1)   0.885246  0.760563  0.818182                61.0\n",
      "2     SMOTE + Random Forest   1.000000  0.352113  0.520833                25.0\n",
      "\n",
      "Tabla de comparaci√≥n:\n",
      "                    Enfoque  Precision  Recall  F1_Score  Fraudes_Detectados\n",
      "0  Baseline (threshold=0.5)     1.0000  0.1268    0.2250                 9.0\n",
      "1  Threshold Ajustado (0.1)     0.8852  0.7606    0.8182                61.0\n",
      "2     SMOTE + Random Forest     1.0000  0.3521    0.5208                25.0\n",
      "\n",
      "üèÜ MEJOR ENFOQUE: Threshold Ajustado (0.1)\n",
      "üìà F1-Score: 0.8182\n",
      "üéØ Recall: 0.7606\n",
      "‚öñÔ∏è Precision: 0.8852\n",
      "\n",
      "‚úÖ Comparaci√≥n completada\n"
     ]
    }
   ],
   "source": [
    "# Comparaci√≥n de enfoques\n",
    "print(\"üîç COMPARACI√ìN DE ENFOQUES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Enfoque': [\n",
    "        'Baseline (threshold=0.5)',\n",
    "        f'Threshold Ajustado ({best_threshold})',\n",
    "        'SMOTE + Random Forest'\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_baseline,\n",
    "        best_result['precision'],\n",
    "        precision_smote\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_baseline,\n",
    "        best_result['recall'],\n",
    "        recall_smote\n",
    "    ],\n",
    "    'F1_Score': [  # CORREGIDO: Sin gui√≥n\n",
    "        f1_baseline,\n",
    "        best_result['f1'],\n",
    "        f1_smote\n",
    "    ],\n",
    "    'Fraudes_Detectados': [\n",
    "        y_pred_baseline.sum(),\n",
    "        best_result['detected'],\n",
    "        y_pred_smote.sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# DEBUG: Ver columnas exactas del DataFrame\n",
    "print(\"DEBUG - Columnas disponibles en comparison:\")\n",
    "print(comparison.columns.tolist())\n",
    "print(\"\\nPrimeras filas del DataFrame:\")\n",
    "print(comparison.head())\n",
    "\n",
    "print(\"\\nTabla de comparaci√≥n:\")\n",
    "print(comparison.round(4))\n",
    "\n",
    "# Determinar mejor enfoque - CORREGIDO\n",
    "best_approach_idx = comparison['F1_Score'].idxmax()  # Sin gui√≥n\n",
    "best_approach = comparison.iloc[best_approach_idx]['Enfoque']\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR ENFOQUE: {best_approach}\")\n",
    "print(f\"üìà F1-Score: {comparison.iloc[best_approach_idx]['F1_Score']:.4f}\")\n",
    "print(f\"üéØ Recall: {comparison.iloc[best_approach_idx]['Recall']:.4f}\")\n",
    "print(f\"‚öñÔ∏è Precision: {comparison.iloc[best_approach_idx]['Precision']:.4f}\")\n",
    "print(\"\\n‚úÖ Comparaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar mejor modelo y configuraci√≥n\n",
    "print(\"üíæ GUARDANDO MEJOR CONFIGURACI√ìN\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Determinar cual guardar basado en F1-Score\n",
    "if f1_smote >= best_result['f1']:\n",
    "    # SMOTE es mejor\n",
    "    best_model = rf_smote\n",
    "    best_method = \"SMOTE\"\n",
    "    best_f1_final = f1_smote\n",
    "    best_recall_final = recall_smote\n",
    "    best_precision_final = precision_smote\n",
    "    model_path = '../models/improved_recall_smote_model.pkl'\n",
    "else:\n",
    "    # Threshold ajustado es mejor\n",
    "    best_model = simple_model  # Usar el modelo original con threshold\n",
    "    best_method = f\"Threshold {best_threshold}\"\n",
    "    best_f1_final = best_result['f1']\n",
    "    best_recall_final = best_result['recall']\n",
    "    best_precision_final = best_result['precision']\n",
    "    model_path = '../models/improved_recall_threshold_model.pkl'\n",
    "    \n",
    "    # Guardar tambi√©n el threshold\n",
    "    import json\n",
    "    config = {'best_threshold': float(best_threshold)}\n",
    "    with open('../models/threshold_config.json', 'w') as f:\n",
    "        json.dump(config, f)\n",
    "    print(f\"‚úÖ Threshold guardado: {best_threshold}\")\n",
    "\n",
    "# Crear directorio models si no existe\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úÖ Modelo guardado: {model_path}\")\n",
    "\n",
    "# Calcular mejora porcentual\n",
    "recall_improvement = (best_recall_final / recall_baseline - 1) * 100 if recall_baseline > 0 else 0\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\nüéØ RESUMEN FINAL - MEJORA DE RECALL:\")\n",
    "print(f\"‚Ä¢ M√©todo ganador: {best_method}\")\n",
    "print(f\"‚Ä¢ F1-Score: {best_f1_final:.4f} (vs {f1_baseline:.4f} baseline)\")\n",
    "print(f\"‚Ä¢ Recall: {best_recall_final:.4f} (vs {recall_baseline:.4f} baseline)\")\n",
    "print(f\"‚Ä¢ Precision: {best_precision_final:.4f} (vs {precision_baseline:.4f} baseline)\")\n",
    "print(f\"‚Ä¢ Mejora en recall: {recall_improvement:.1f}%\")\n",
    "print(f\"‚Ä¢ Modelo guardado exitosamente\")\n",
    "\n",
    "print(f\"\\nüë®‚Äçüíª Desarrollado por: Ing. Daniel Varela Perez\")\n",
    "print(f\"üìß bedaniele0@gmail.com | üì± +52 55 4189 3428\")\n",
    "print(\"\\n‚úÖ PROCESO COMPLETADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaf8b5-f539-4a10-89ca-367b67b7ac20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
