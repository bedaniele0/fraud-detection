{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# An√°lisis SQL y Exploratorio - Detecci√≥n de Fraude Bancario\n",
    "===========================================================\n",
    "\n",
    "**Autor**: Ing. Daniel Varela Perez  \n",
    "**Email**: bedaniele0@gmail.com  \n",
    "**Tel√©fono**: +52 55 4189 3428  \n",
    "**Fecha**: 24 de Septiembre, 2025  \n",
    "\n",
    "**Objetivo**: An√°lisis profundo usando SQL para identificar patrones de fraude en transacciones bancarias\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Contenido\n",
    "1. [Setup y Carga de Datos](#setup)\n",
    "2. [An√°lisis SQL B√°sico](#sql-basic)\n",
    "3. [Feature Engineering con SQL](#feature-engineering)\n",
    "4. [An√°lisis de Desbalanceo](#imbalance)\n",
    "5. [Visualizaciones](#visualizations)\n",
    "6. [Insights y Conclusiones](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup y Carga de Datos\n",
    "\n",
    "Configuramos el entorno e importamos las librer√≠as necesarias para el an√°lisis SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Informaci√≥n del proyecto\n",
    "print(\"üîç AN√ÅLISIS SQL - DETECCI√ìN DE FRAUDE BANCARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Desarrollado por: Ing. Daniel Varela Perez\")\n",
    "print(f\"üìß Email: bedaniele0@gmail.com\")\n",
    "print(f\"üì± Tel: +52 55 4189 3428\")\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar configuraci√≥n del proyecto\n",
    "sys.path.append('../src/utils')\n",
    "from config import get_config\n",
    "from environment import get_environment\n",
    "\n",
    "config = get_config()\n",
    "env = get_environment()\n",
    "\n",
    "print(\"üìã CONFIGURACI√ìN DEL PROYECTO:\")\n",
    "print(f\"‚Ä¢ Nombre: {config.get('project.name')}\")\n",
    "print(f\"‚Ä¢ Autor: {config.get('project.author')}\")\n",
    "print(f\"‚Ä¢ Entorno: {env.environment}\")\n",
    "print(f\"‚Ä¢ Dataset: {config.get('data.raw_path')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset de fraudes\n",
    "print(\"üìä CARGANDO DATASET...\")\n",
    "\n",
    "# Ruta del dataset\n",
    "dataset_path = '../data/raw/creditcard.csv'\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
    "print(f\"üìè Dimensiones: {df.shape}\")\n",
    "print(f\"üíæ Tama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Informaci√≥n b√°sica\n",
    "print(\"\\nüìã INFORMACI√ìN B√ÅSICA:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sqlite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexi√≥n SQLite en memoria\n",
    "print(\"üóÉÔ∏è CONFIGURANDO BASE DE DATOS SQLite...\")\n",
    "\n",
    "# Conexi√≥n en memoria para an√°lisis r√°pido\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Cargar datos en tabla SQL\n",
    "table_name = 'transactions'\n",
    "df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
    "\n",
    "print(f\"‚úÖ Tabla '{table_name}' creada en SQLite\")\n",
    "print(f\"üìä Registros cargados: {len(df):,}\")\n",
    "\n",
    "# Verificar estructura de la tabla\n",
    "schema_query = f\"PRAGMA table_info({table_name})\"\n",
    "schema_df = pd.read_sql_query(schema_query, conn)\n",
    "print(f\"\\nüèóÔ∏è ESQUEMA DE LA TABLA:\")\n",
    "print(schema_df[['name', 'type']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sql-basic",
   "metadata": {},
   "source": [
    "## 2. An√°lisis SQL B√°sico\n",
    "\n",
    "Realizamos consultas SQL fundamentales para entender la distribuci√≥n y caracter√≠sticas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fraud-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Distribuci√≥n de fraudes vs transacciones normales\n",
    "fraud_dist_query = \"\"\"\n",
    "SELECT \n",
    "    Class,\n",
    "    COUNT(*) as total_transactions,\n",
    "    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM transactions), 4) as percentage,\n",
    "    CASE \n",
    "        WHEN Class = 0 THEN 'Normal'\n",
    "        WHEN Class = 1 THEN 'Fraud'\n",
    "    END as transaction_type\n",
    "FROM transactions\n",
    "GROUP BY Class\n",
    "ORDER BY Class;\n",
    "\"\"\"\n",
    "\n",
    "fraud_distribution = pd.read_sql_query(fraud_dist_query, conn)\n",
    "print(\"üìä DISTRIBUCI√ìN DE FRAUDES:\")\n",
    "print(fraud_distribution)\n",
    "\n",
    "# Guardar resultado\n",
    "fraud_rate = fraud_distribution[fraud_distribution['Class'] == 1]['percentage'].iloc[0]\n",
    "print(f\"\\nüéØ TASA DE FRAUDE: {fraud_rate}%\")\n",
    "print(f\"üö® TOTAL DE FRAUDES: {fraud_distribution[fraud_distribution['Class'] == 1]['total_transactions'].iloc[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amount-analysis",
   "metadata": {},
   "outputs": [],
   "source": "# Query 2: Estad√≠sticas por monto de transacci√≥n (SQLite compatible)\namount_stats_query = \"\"\"\nSELECT \n    Class,\n    CASE \n        WHEN Class = 0 THEN 'Normal'\n        WHEN Class = 1 THEN 'Fraud'\n    END as transaction_type,\n    COUNT(*) as count,\n    ROUND(AVG(Amount), 2) as avg_amount,\n    ROUND(MIN(Amount), 2) as min_amount,\n    ROUND(MAX(Amount), 2) as max_amount\nFROM transactions\nGROUP BY Class\nORDER BY Class;\n\"\"\"\n\namount_stats = pd.read_sql_query(amount_stats_query, conn)\nprint(\"üí∞ ESTAD√çSTICAS DE MONTOS POR TIPO:\")\nprint(amount_stats)\n\n# Calcular percentiles usando pandas para complementar\nnormal_amounts = df[df['Class'] == 0]['Amount']\nfraud_amounts = df[df['Class'] == 1]['Amount']\n\nprint(\"\\nüìä PERCENTILES ADICIONALES:\")\nprint(f\"Normal - Q25: ${normal_amounts.quantile(0.25):.2f}, Mediana: ${normal_amounts.median():.2f}, Q75: ${normal_amounts.quantile(0.75):.2f}\")\nprint(f\"Fraude - Q25: ${fraud_amounts.quantile(0.25):.2f}, Mediana: ${fraud_amounts.median():.2f}, Q75: ${fraud_amounts.quantile(0.75):.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: An√°lisis temporal (variable Time)\n",
    "temporal_analysis_query = \"\"\"\n",
    "WITH time_stats AS (\n",
    "    SELECT \n",
    "        Class,\n",
    "        Time,\n",
    "        Amount,\n",
    "        -- Convertir Time a horas (Time est√° en segundos desde primer transacci√≥n)\n",
    "        ROUND(Time / 3600.0, 2) as hour_from_start,\n",
    "        -- Crear buckets de tiempo para an√°lisis\n",
    "        CASE \n",
    "            WHEN Time < 86400 THEN 'Day_1'\n",
    "            WHEN Time < 172800 THEN 'Day_2' \n",
    "            ELSE 'Later'\n",
    "        END as time_bucket\n",
    "    FROM transactions\n",
    ")\n",
    "SELECT \n",
    "    time_bucket,\n",
    "    Class,\n",
    "    CASE WHEN Class = 0 THEN 'Normal' ELSE 'Fraud' END as type,\n",
    "    COUNT(*) as transaction_count,\n",
    "    ROUND(AVG(Amount), 2) as avg_amount,\n",
    "    ROUND(AVG(hour_from_start), 2) as avg_hour\n",
    "FROM time_stats\n",
    "GROUP BY time_bucket, Class\n",
    "ORDER BY time_bucket, Class;\n",
    "\"\"\"\n",
    "\n",
    "temporal_analysis = pd.read_sql_query(temporal_analysis_query, conn)\n",
    "print(\"‚è∞ AN√ÅLISIS TEMPORAL:\")\n",
    "print(temporal_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-transactions",
   "metadata": {},
   "outputs": [],
   "source": "# Query 4: Top 10 transacciones por monto (normales y fraudulentas)\ntop_transactions_query = \"\"\"\nSELECT type, Amount, Time, Class FROM (\n    SELECT 'Normal' as type, Amount, Time, Class\n    FROM transactions \n    WHERE Class = 0 \n    ORDER BY Amount DESC \n    LIMIT 10\n\n    UNION ALL\n\n    SELECT 'Fraud' as type, Amount, Time, Class\n    FROM transactions \n    WHERE Class = 1 \n    ORDER BY Amount DESC \n    LIMIT 10\n) ORDER BY type, Amount DESC;\n\"\"\"\n\ntop_transactions = pd.read_sql_query(top_transactions_query, conn)\nprint(\"üîù TOP TRANSACCIONES POR MONTO:\")\nprint(top_transactions)"
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering con SQL\n",
    "\n",
    "Creamos nuevas caracter√≠sticas usando window functions y an√°lisis temporal avanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velocity-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering 1: Velocidad de transacciones\n",
    "# Contar transacciones en ventanas de tiempo\n",
    "velocity_query = \"\"\"\n",
    "WITH transaction_velocity AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Transacciones en la √∫ltima hora (3600 segundos)\n",
    "        COUNT(*) OVER (\n",
    "            ORDER BY Time \n",
    "            RANGE BETWEEN 3600 PRECEDING AND CURRENT ROW\n",
    "        ) as transactions_last_hour,\n",
    "        \n",
    "        -- Transacciones en las √∫ltimas 6 horas \n",
    "        COUNT(*) OVER (\n",
    "            ORDER BY Time \n",
    "            RANGE BETWEEN 21600 PRECEDING AND CURRENT ROW\n",
    "        ) as transactions_last_6hours,\n",
    "        \n",
    "        -- Tiempo desde la transacci√≥n anterior\n",
    "        Time - LAG(Time, 1, Time) OVER (ORDER BY Time) as time_since_last\n",
    "    FROM transactions\n",
    "    ORDER BY Time\n",
    ")\n",
    "SELECT \n",
    "    Class,\n",
    "    CASE WHEN Class = 0 THEN 'Normal' ELSE 'Fraud' END as type,\n",
    "    ROUND(AVG(transactions_last_hour), 2) as avg_velocity_1h,\n",
    "    ROUND(AVG(transactions_last_6hours), 2) as avg_velocity_6h,\n",
    "    ROUND(AVG(time_since_last), 2) as avg_time_between,\n",
    "    COUNT(*) as total_transactions\n",
    "FROM transaction_velocity\n",
    "GROUP BY Class\n",
    "ORDER BY Class;\n",
    "\"\"\"\n",
    "\n",
    "velocity_analysis = pd.read_sql_query(velocity_query, conn)\n",
    "print(\"‚ö° AN√ÅLISIS DE VELOCIDAD DE TRANSACCIONES:\")\n",
    "print(velocity_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amount-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering 2: Frecuencia por rangos de monto\n",
    "amount_frequency_query = \"\"\"\n",
    "WITH amount_ranges AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        CASE \n",
    "            WHEN Amount = 0 THEN 'Zero'\n",
    "            WHEN Amount > 0 AND Amount <= 10 THEN 'Very_Low (0-10)'\n",
    "            WHEN Amount > 10 AND Amount <= 50 THEN 'Low (10-50)'\n",
    "            WHEN Amount > 50 AND Amount <= 100 THEN 'Medium (50-100)'\n",
    "            WHEN Amount > 100 AND Amount <= 500 THEN 'High (100-500)'\n",
    "            WHEN Amount > 500 AND Amount <= 1000 THEN 'Very_High (500-1000)'\n",
    "            ELSE 'Extreme (>1000)'\n",
    "        END as amount_range\n",
    "    FROM transactions\n",
    ")\n",
    "SELECT \n",
    "    amount_range,\n",
    "    Class,\n",
    "    COUNT(*) as frequency,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY Class), 2) as percentage_within_class,\n",
    "    ROUND(AVG(Amount), 2) as avg_amount_in_range,\n",
    "    ROUND(SUM(CASE WHEN Class = 1 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as fraud_rate_in_range\n",
    "FROM amount_ranges\n",
    "GROUP BY amount_range, Class\n",
    "ORDER BY \n",
    "    CASE amount_range\n",
    "        WHEN 'Zero' THEN 1\n",
    "        WHEN 'Very_Low (0-10)' THEN 2\n",
    "        WHEN 'Low (10-50)' THEN 3\n",
    "        WHEN 'Medium (50-100)' THEN 4\n",
    "        WHEN 'High (100-500)' THEN 5\n",
    "        WHEN 'Very_High (500-1000)' THEN 6\n",
    "        WHEN 'Extreme (>1000)' THEN 7\n",
    "    END, Class;\n",
    "\"\"\"\n",
    "\n",
    "amount_frequency = pd.read_sql_query(amount_frequency_query, conn)\n",
    "print(\"üíµ AN√ÅLISIS DE FRECUENCIA POR RANGOS DE MONTO:\")\n",
    "print(amount_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anomaly-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering 3: Detecci√≥n de patrones an√≥malos\n",
    "anomaly_patterns_query = \"\"\"\n",
    "WITH anomaly_features AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Z-score para Amount\n",
    "        ROUND((Amount - AVG(Amount) OVER ()) / NULLIF(STDDEV(Amount) OVER (), 0), 3) as amount_zscore,\n",
    "        \n",
    "        -- Ranking por monto\n",
    "        NTILE(100) OVER (ORDER BY Amount) as amount_percentile,\n",
    "        \n",
    "        -- Diferencia con monto promedio en ventana de tiempo\n",
    "        ROUND(Amount - AVG(Amount) OVER (\n",
    "            ORDER BY Time \n",
    "            RANGE BETWEEN 3600 PRECEDING AND 3600 FOLLOWING\n",
    "        ), 2) as amount_diff_local_avg,\n",
    "        \n",
    "        -- Transacciones consecutivas r√°pidas (< 60 segundos)\n",
    "        CASE \n",
    "            WHEN Time - LAG(Time, 1, 0) OVER (ORDER BY Time) < 60 THEN 1\n",
    "            ELSE 0\n",
    "        END as is_rapid_transaction\n",
    "        \n",
    "    FROM transactions\n",
    ")\n",
    "SELECT \n",
    "    Class,\n",
    "    CASE WHEN Class = 0 THEN 'Normal' ELSE 'Fraud' END as type,\n",
    "    \n",
    "    -- Estad√≠sticas de Z-score\n",
    "    ROUND(AVG(ABS(amount_zscore)), 3) as avg_abs_zscore,\n",
    "    ROUND(AVG(amount_zscore), 3) as avg_zscore,\n",
    "    \n",
    "    -- Transacciones en percentiles extremos (>95 o <5)\n",
    "    ROUND(SUM(CASE WHEN amount_percentile > 95 OR amount_percentile < 5 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as pct_extreme_amounts,\n",
    "    \n",
    "    -- Diferencia promedio con contexto local\n",
    "    ROUND(AVG(ABS(amount_diff_local_avg)), 2) as avg_local_deviation,\n",
    "    \n",
    "    -- Porcentaje de transacciones r√°pidas\n",
    "    ROUND(AVG(is_rapid_transaction) * 100, 2) as pct_rapid_transactions,\n",
    "    \n",
    "    COUNT(*) as total_transactions\n",
    "    \n",
    "FROM anomaly_features\n",
    "GROUP BY Class\n",
    "ORDER BY Class;\n",
    "\"\"\"\n",
    "\n",
    "anomaly_patterns = pd.read_sql_query(anomaly_patterns_query, conn)\n",
    "print(\"üîç DETECCI√ìN DE PATRONES AN√ìMALOS:\")\n",
    "print(anomaly_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imbalance",
   "metadata": {},
   "source": [
    "## 4. An√°lisis de Desbalanceo\n",
    "\n",
    "Analizamos en profundidad el desbalanceo del dataset y definimos estrategias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-imbalance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado del desbalanceo\n",
    "imbalance_query = \"\"\"\n",
    "WITH class_stats AS (\n",
    "    SELECT \n",
    "        Class,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM transactions), 6) as percentage\n",
    "    FROM transactions\n",
    "    GROUP BY Class\n",
    "),\n",
    "ratios AS (\n",
    "    SELECT \n",
    "        (SELECT count FROM class_stats WHERE Class = 0) as normal_count,\n",
    "        (SELECT count FROM class_stats WHERE Class = 1) as fraud_count,\n",
    "        ROUND(\n",
    "            (SELECT count FROM class_stats WHERE Class = 0) * 1.0 / \n",
    "            (SELECT count FROM class_stats WHERE Class = 1), 2\n",
    "        ) as imbalance_ratio\n",
    ")\n",
    "SELECT \n",
    "    cs.*,\n",
    "    CASE WHEN Class = 0 THEN 'Normal' ELSE 'Fraud' END as type,\n",
    "    r.imbalance_ratio,\n",
    "    -- Calcular cu√°ntos ejemplos necesitar√≠amos para diferentes estrategias\n",
    "    CASE \n",
    "        WHEN Class = 1 THEN \n",
    "            ROUND(r.normal_count * 0.1) - count  -- Para ratio 10:1\n",
    "        ELSE NULL\n",
    "    END as samples_needed_10_to_1,\n",
    "    CASE \n",
    "        WHEN Class = 1 THEN \n",
    "            ROUND(r.normal_count * 0.05) - count  -- Para ratio 20:1\n",
    "        ELSE NULL\n",
    "    END as samples_needed_20_to_1\n",
    "FROM class_stats cs\n",
    "CROSS JOIN ratios r\n",
    "ORDER BY Class;\n",
    "\"\"\"\n",
    "\n",
    "imbalance_analysis = pd.read_sql_query(imbalance_query, conn)\n",
    "print(\"‚öñÔ∏è AN√ÅLISIS DETALLADO DE DESBALANCEO:\")\n",
    "print(imbalance_analysis)\n",
    "\n",
    "# Extraer informaci√≥n clave\n",
    "imbalance_ratio = imbalance_analysis['imbalance_ratio'].iloc[0]\n",
    "fraud_count = imbalance_analysis[imbalance_analysis['Class'] == 1]['count'].iloc[0]\n",
    "normal_count = imbalance_analysis[imbalance_analysis['Class'] == 0]['count'].iloc[0]\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS CLAVE:\")\n",
    "print(f\"‚Ä¢ Ratio de desbalanceo: {imbalance_ratio}:1 (Normal:Fraud)\")\n",
    "print(f\"‚Ä¢ Transacciones normales: {normal_count:,}\")\n",
    "print(f\"‚Ä¢ Transacciones fraudulentas: {fraud_count:,}\")\n",
    "print(f\"‚Ä¢ Tasa de fraude: {fraud_count/(fraud_count + normal_count)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "percentile-analysis",
   "metadata": {},
   "outputs": [],
   "source": "# An√°lisis por percentiles usando pandas (SQLite no soporta PERCENTILE)\nprint(\"üìä AN√ÅLISIS POR PERCENTILES:\")\n\n# Funci√≥n para calcular percentiles por clase\ndef calculate_percentiles(data, class_label):\n    amounts = data[data['Class'] == class_label]['Amount']\n    return {\n        'Class': class_label,\n        'type': 'Normal' if class_label == 0 else 'Fraud',\n        'count': len(amounts),\n        'min_amount': amounts.min(),\n        'p01_amount': amounts.quantile(0.01),\n        'p05_amount': amounts.quantile(0.05),\n        'p10_amount': amounts.quantile(0.10),\n        'p25_amount': amounts.quantile(0.25),\n        'p50_amount': amounts.quantile(0.50),\n        'p75_amount': amounts.quantile(0.75),\n        'p90_amount': amounts.quantile(0.90),\n        'p95_amount': amounts.quantile(0.95),\n        'p99_amount': amounts.quantile(0.99),\n        'max_amount': amounts.max(),\n        'mean_amount': amounts.mean(),\n        'std_amount': amounts.std()\n    }\n\n# Calcular percentiles para ambas clases\npercentiles_normal = calculate_percentiles(df, 0)\npercentiles_fraud = calculate_percentiles(df, 1)\n\n# Convertir a DataFrame para mejor visualizaci√≥n\npercentile_analysis = pd.DataFrame([percentiles_normal, percentiles_fraud])\n\n# Mostrar an√°lisis transpuesto\nprint(percentile_analysis.round(2).T)\n\n# Comparar diferencias clave\nprint(\"\\nüéØ COMPARACI√ìN CLAVE:\")\nprint(f\"‚Ä¢ Mediana Normal: ${percentiles_normal['p50_amount']:.2f}\")\nprint(f\"‚Ä¢ Mediana Fraude: ${percentiles_fraud['p50_amount']:.2f}\")\nprint(f\"‚Ä¢ Media Normal: ${percentiles_normal['mean_amount']:.2f}\")\nprint(f\"‚Ä¢ Media Fraude: ${percentiles_fraud['mean_amount']:.2f}\")\nprint(f\"‚Ä¢ P95 Normal: ${percentiles_normal['p95_amount']:.2f}\")\nprint(f\"‚Ä¢ P95 Fraude: ${percentiles_fraud['p95_amount']:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "visualizations",
   "metadata": {},
   "source": [
    "## 5. Visualizaciones\n",
    "\n",
    "Creamos visualizaciones para identificar patrones visuales en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amount-distribution-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n 1: Distribuci√≥n de montos (Normal vs Fraud)\n",
    "print(\"üìä Creando visualizaci√≥n de distribuci√≥n de montos...\")\n",
    "\n",
    "# Preparar datos\n",
    "normal_amounts = df[df['Class'] == 0]['Amount']\n",
    "fraud_amounts = df[df['Class'] == 1]['Amount']\n",
    "\n",
    "# Crear subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('An√°lisis de Distribuci√≥n de Montos - Normal vs Fraude\\nDesarrollado por: Ing. Daniel Varela Perez', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histograma comparativo\n",
    "axes[0,0].hist(normal_amounts, bins=50, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "axes[0,0].hist(fraud_amounts, bins=50, alpha=0.7, label='Fraude', color='red', density=True)\n",
    "axes[0,0].set_xlabel('Monto ($)')\n",
    "axes[0,0].set_ylabel('Densidad')\n",
    "axes[0,0].set_title('Distribuci√≥n de Montos (Escala Normal)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Histograma en escala log\n",
    "axes[0,1].hist(normal_amounts[normal_amounts > 0], bins=50, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "axes[0,1].hist(fraud_amounts[fraud_amounts > 0], bins=50, alpha=0.7, label='Fraude', color='red', density=True)\n",
    "axes[0,1].set_xscale('log')\n",
    "axes[0,1].set_xlabel('Monto ($) - Escala Log')\n",
    "axes[0,1].set_ylabel('Densidad')\n",
    "axes[0,1].set_title('Distribuci√≥n de Montos (Escala Logar√≠tmica)')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Box plot comparativo\n",
    "box_data = [normal_amounts, fraud_amounts]\n",
    "box_labels = ['Normal', 'Fraude']\n",
    "bp = axes[1,0].boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('blue')\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "axes[1,0].set_ylabel('Monto ($)')\n",
    "axes[1,0].set_title('Box Plot de Montos por Clase')\n",
    "\n",
    "# Violin plot\n",
    "violin_data = pd.DataFrame({\n",
    "    'Amount': np.concatenate([normal_amounts.values[:5000], fraud_amounts.values]),  # Sample para performance\n",
    "    'Class': ['Normal']*5000 + ['Fraud']*len(fraud_amounts)\n",
    "})\n",
    "sns.violinplot(data=violin_data, x='Class', y='Amount', ax=axes[1,1])\n",
    "axes[1,1].set_title('Violin Plot de Montos por Clase')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/amount_distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaci√≥n guardada en reports/figures/amount_distribution_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-patterns-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n 2: Patrones temporales\n",
    "print(\"‚è∞ Creando visualizaci√≥n de patrones temporales...\")\n",
    "\n",
    "# Preparar datos temporales\n",
    "df_temp = df.copy()\n",
    "df_temp['Hours'] = df_temp['Time'] / 3600  # Convertir a horas\n",
    "df_temp['Days'] = df_temp['Hours'] / 24    # Convertir a d√≠as\n",
    "\n",
    "# Crear visualizaci√≥n temporal\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('An√°lisis de Patrones Temporales\\nDesarrollado por: Ing. Daniel Varela Perez', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribuci√≥n temporal de fraudes\n",
    "normal_times = df_temp[df_temp['Class'] == 0]['Hours']\n",
    "fraud_times = df_temp[df_temp['Class'] == 1]['Hours']\n",
    "\n",
    "axes[0,0].hist(normal_times, bins=100, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "axes[0,0].hist(fraud_times, bins=100, alpha=0.7, label='Fraude', color='red', density=True)\n",
    "axes[0,0].set_xlabel('Tiempo (Horas desde inicio)')\n",
    "axes[0,0].set_ylabel('Densidad')\n",
    "axes[0,0].set_title('Distribuci√≥n Temporal de Transacciones')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Tasa de fraude por hora del d√≠a (simulada)\n",
    "df_temp['Hour_of_Day'] = (df_temp['Hours'] % 24).astype(int)\n",
    "hourly_fraud_rate = df_temp.groupby('Hour_of_Day').agg({\n",
    "    'Class': ['count', 'sum']\n",
    "}).reset_index()\n",
    "hourly_fraud_rate.columns = ['Hour', 'Total', 'Frauds']\n",
    "hourly_fraud_rate['Fraud_Rate'] = hourly_fraud_rate['Frauds'] / hourly_fraud_rate['Total'] * 100\n",
    "\n",
    "axes[0,1].bar(hourly_fraud_rate['Hour'], hourly_fraud_rate['Fraud_Rate'], color='orange', alpha=0.7)\n",
    "axes[0,1].set_xlabel('Hora del D√≠a')\n",
    "axes[0,1].set_ylabel('Tasa de Fraude (%)')\n",
    "axes[0,1].set_title('Tasa de Fraude por Hora del D√≠a')\n",
    "\n",
    "# Evoluci√≥n temporal de montos\n",
    "# Agrupar por ventanas de tiempo\n",
    "df_temp['Time_Window'] = (df_temp['Hours'] // 6).astype(int)  # Ventanas de 6 horas\n",
    "time_amount_stats = df_temp.groupby(['Time_Window', 'Class'])['Amount'].mean().unstack(fill_value=0)\n",
    "\n",
    "axes[1,0].plot(time_amount_stats.index, time_amount_stats[0], label='Normal', color='blue', linewidth=2)\n",
    "axes[1,0].plot(time_amount_stats.index, time_amount_stats[1], label='Fraude', color='red', linewidth=2)\n",
    "axes[1,0].set_xlabel('Ventana de Tiempo (6h)')\n",
    "axes[1,0].set_ylabel('Monto Promedio ($)')\n",
    "axes[1,0].set_title('Evoluci√≥n Temporal de Montos Promedio')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Heatmap de fraudes por d√≠a y hora\n",
    "df_temp['Day'] = (df_temp['Hours'] // 24).astype(int)\n",
    "df_sample = df_temp[df_temp['Day'] < 7]  # Primeros 7 d√≠as\n",
    "pivot_data = df_sample.groupby(['Day', 'Hour_of_Day'])['Class'].sum().unstack(fill_value=0)\n",
    "\n",
    "sns.heatmap(pivot_data, cmap='Reds', ax=axes[1,1], cbar_kws={'label': 'N√∫mero de Fraudes'})\n",
    "axes[1,1].set_xlabel('Hora del D√≠a')\n",
    "axes[1,1].set_ylabel('D√≠a')\n",
    "axes[1,1].set_title('Heatmap de Fraudes (Primeros 7 d√≠as)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/temporal_patterns_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaci√≥n guardada en reports/figures/temporal_patterns_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n 3: Correlaciones entre variables\n",
    "print(\"üîó Creando an√°lisis de correlaciones...\")\n",
    "\n",
    "# Seleccionar subset de variables para correlaci√≥n (V1-V10 + Amount + Class)\n",
    "correlation_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'Amount', 'Class']\n",
    "corr_data = df[correlation_cols]\n",
    "\n",
    "# Calcular matriz de correlaci√≥n\n",
    "correlation_matrix = corr_data.corr()\n",
    "\n",
    "# Crear visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('An√°lisis de Correlaciones\\nDesarrollado por: Ing. Daniel Varela Perez', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Heatmap de correlaciones\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0, \n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Matriz de Correlaci√≥n (Variables V1-V10 + Amount + Class)')\n",
    "\n",
    "# Correlaciones espec√≠ficas con la variable Class\n",
    "class_correlations = correlation_matrix['Class'].drop('Class').sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Bar plot de correlaciones con Class\n",
    "colors = ['red' if x < 0 else 'green' for x in class_correlations.values]\n",
    "axes[1].bar(range(len(class_correlations)), class_correlations.values, color=colors, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(class_correlations)))\n",
    "axes[1].set_xticklabels(class_correlations.index, rotation=45)\n",
    "axes[1].set_ylabel('Correlaci√≥n con Class')\n",
    "axes[1].set_title('Correlaciones con Variable Objetivo (Class)')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar correlaciones m√°s significativas\n",
    "print(\"\\nüéØ CORRELACIONES M√ÅS SIGNIFICATIVAS CON FRAUDE:\")\n",
    "for var, corr in class_correlations.head(10).items():\n",
    "    print(f\"‚Ä¢ {var}: {corr:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Visualizaci√≥n guardada en reports/figures/correlation_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 6. Insights y Conclusiones\n",
    "\n",
    "Documentamos los hallazgos principales y patrones identificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "key-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recopilar insights clave del an√°lisis\n",
    "print(\"üéØ INSIGHTS CLAVE IDENTIFICADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Insight 1: Desbalanceo extremo\n",
    "print(f\"\\n1. üìä DESBALANCEO EXTREMO:\")\n",
    "print(f\"   ‚Ä¢ Ratio: {imbalance_ratio:.1f}:1 (Normal:Fraude)\")\n",
    "print(f\"   ‚Ä¢ Solo {fraud_rate:.4f}% de transacciones son fraude\")\n",
    "print(f\"   ‚Ä¢ Requiere t√©cnicas especializadas de balanceado\")\n",
    "\n",
    "# Insight 2: Diferencias en montos\n",
    "normal_avg = amount_stats[amount_stats['Class'] == 0]['avg_amount'].iloc[0]\n",
    "fraud_avg = amount_stats[amount_stats['Class'] == 1]['avg_amount'].iloc[0]\n",
    "print(f\"\\n2. üí∞ PATRONES DE MONTOS:\")\n",
    "print(f\"   ‚Ä¢ Monto promedio normal: ${normal_avg:.2f}\")\n",
    "print(f\"   ‚Ä¢ Monto promedio fraude: ${fraud_avg:.2f}\")\n",
    "print(f\"   ‚Ä¢ Diferencia: {abs(normal_avg - fraud_avg):.2f} ({'menor' if fraud_avg < normal_avg else 'mayor'} en fraudes)\")\n",
    "\n",
    "# Insight 3: Patrones temporales\n",
    "print(f\"\\n3. ‚è∞ DISTRIBUCI√ìN TEMPORAL:\")\n",
    "print(f\"   ‚Ä¢ Fraudes distribuidos a lo largo del tiempo\")\n",
    "print(f\"   ‚Ä¢ No se observan concentraciones claras por hora\")\n",
    "print(f\"   ‚Ä¢ Patr√≥n temporal relativamente uniforme\")\n",
    "\n",
    "# Insight 4: Variables m√°s correlacionadas\n",
    "top_corr_vars = abs(class_correlations).head(3)\n",
    "print(f\"\\n4. üîó VARIABLES M√ÅS PREDICTIVAS:\")\n",
    "for var, corr in top_corr_vars.items():\n",
    "    direction = \"positiva\" if class_correlations[var] > 0 else \"negativa\"\n",
    "    print(f\"   ‚Ä¢ {var}: {corr:.4f} (correlaci√≥n {direction})\")\n",
    "\n",
    "# Insight 5: Anomal√≠as detectadas\n",
    "normal_anomalies = anomaly_patterns[anomaly_patterns['Class'] == 0]\n",
    "fraud_anomalies = anomaly_patterns[anomaly_patterns['Class'] == 1]\n",
    "print(f\"\\n5. üö® DETECCI√ìN DE ANOMAL√çAS:\")\n",
    "print(f\"   ‚Ä¢ Transacciones normales con montos extremos: {normal_anomalies['pct_extreme_amounts'].iloc[0]:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Transacciones fraude con montos extremos: {fraud_anomalies['pct_extreme_amounts'].iloc[0]:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Transacciones r√°pidas en fraudes: {fraud_anomalies['pct_rapid_transactions'].iloc[0]:.2f}%\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"üë®‚Äçüíª An√°lisis realizado por: Ing. Daniel Varela Perez\")\n",
    "print(f\"üìß Email: bedaniele0@gmail.com | üì± Tel: +52 55 4189 3428\")\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendaciones para el modelado\n",
    "print(\"üöÄ RECOMENDACIONES PARA MODELADO ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. üìä ESTRATEGIAS DE BALANCEADO:\")\n",
    "print(\"   ‚Ä¢ Usar SMOTE o ADASYN para oversampling\")\n",
    "print(\"   ‚Ä¢ Considerar undersampling de mayor√≠a\")\n",
    "print(\"   ‚Ä¢ Implementar ensemble con diferentes ratios\")\n",
    "print(\"   ‚Ä¢ Usar cost-sensitive learning\")\n",
    "\n",
    "print(\"\\n2. üéØ M√âTRICAS DE EVALUACI√ìN:\")\n",
    "print(\"   ‚Ä¢ Priorizar Precision, Recall y F1-Score\")\n",
    "print(\"   ‚Ä¢ Usar AUC-ROC y AUC-PR\")\n",
    "print(\"   ‚Ä¢ Evitar Accuracy como m√©trica principal\")\n",
    "print(\"   ‚Ä¢ Implementar matrices de confusi√≥n detalladas\")\n",
    "\n",
    "print(\"\\n3. üîß FEATURE ENGINEERING:\")\n",
    "print(\"   ‚Ä¢ Variables temporales (velocity features)\")\n",
    "print(\"   ‚Ä¢ Agregaciones por ventanas de tiempo\")\n",
    "print(\"   ‚Ä¢ Ratios entre variables existentes\")\n",
    "print(\"   ‚Ä¢ Interacciones entre variables V1-V28\")\n",
    "\n",
    "print(\"\\n4. ü§ñ ALGORITMOS RECOMENDADOS:\")\n",
    "print(\"   ‚Ä¢ XGBoost (excelente para desbalanceados)\")\n",
    "print(\"   ‚Ä¢ LightGBM (r√°pido y eficiente)\")\n",
    "print(\"   ‚Ä¢ Random Forest con class_weight='balanced'\")\n",
    "print(\"   ‚Ä¢ Ensemble de m√∫ltiples algoritmos\")\n",
    "\n",
    "print(\"\\n5. ‚úÖ VALIDACI√ìN:\")\n",
    "print(\"   ‚Ä¢ Usar StratifiedKFold para mantener distribuci√≥n\")\n",
    "print(\"   ‚Ä¢ Time-based split si hay orden temporal\")\n",
    "print(\"   ‚Ä¢ Validar en datos completamente separados\")\n",
    "print(\"   ‚Ä¢ Monitorear drift en producci√≥n\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"üéØ OBJETIVO: Precision > 99.5% y Recall > 85%\")\n",
    "print(f\"üìä Dataset: {len(df):,} transacciones, {fraud_count:,} fraudes\")\n",
    "print(f\"‚ö° Listo para Fase 2: Feature Engineering y Modelado\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar conexiones y guardar resultados\n",
    "print(\"üßπ LIMPIANDO RECURSOS...\")\n",
    "\n",
    "# Cerrar conexi√≥n SQLite\n",
    "conn.close()\n",
    "print(\"‚úÖ Conexi√≥n SQLite cerrada\")\n",
    "\n",
    "# Crear resumen de resultados\n",
    "summary_results = {\n",
    "    'dataset_info': {\n",
    "        'total_transactions': len(df),\n",
    "        'fraud_transactions': fraud_count,\n",
    "        'normal_transactions': normal_count,\n",
    "        'fraud_rate_percent': fraud_rate,\n",
    "        'imbalance_ratio': imbalance_ratio\n",
    "    },\n",
    "    'amount_statistics': {\n",
    "        'normal_avg_amount': float(normal_avg),\n",
    "        'fraud_avg_amount': float(fraud_avg)\n",
    "    },\n",
    "    'top_correlations': dict(class_correlations.head(5)),\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'analyst': 'Ing. Daniel Varela Perez',\n",
    "    'contact': 'bedaniele0@gmail.com'\n",
    "}\n",
    "\n",
    "# Guardar resultados\n",
    "import json\n",
    "with open('../reports/sql_eda_results.json', 'w') as f:\n",
    "    json.dump(summary_results, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Resultados guardados en reports/sql_eda_results.json\")\n",
    "print(\"üìä An√°lisis SQL completado exitosamente\")\n",
    "print(\"üöÄ Listo para continuar con Feature Engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}